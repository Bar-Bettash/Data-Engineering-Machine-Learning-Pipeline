# Data Engineering Machine Learning Pipeline


---

## ğŸš€ Project Goals

- âœ… Automate fetching real-time trending YouTube data via the Kaggle API
- âœ… Perform robust ML-based imputation for missing data
- âœ… Store and serve cleaned data in PostgreSQL and AWS S3
- âœ… Orchestrate the end-to-end pipeline using Airflow
- âœ… Dockerize the pipeline for portability and reproducibility
- âœ… Showcase real-world data engineering skills in a GitHub-ready project

---

## ğŸ”„ Data Pipeline Overview

### 1. ğŸ§² Data Ingestion

- **Script:** `scripts/fetch_data.py`
- **Source:** Kaggle Dataset [`datasnaek/youtube-new`](https://www.kaggle.com/datasets/datasnaek/youtube-new)
- **Auth:** `kaggle.json` via `/root/.kaggle` (set by environment variable `KAGGLE_CONFIG_DIR`)
- **Output:** Downloads and extracts country-wise `.csv` files â†’ standardizes into `youtube_trending.csv`

### 2. ğŸ§¼ Data Cleaning & ML-Based Imputation

- **Script:** `notebooks/DataCleaning.py`
- **Library Used:** `scikit-learn`, `pandas`, `joblib`
- **Workflow:**
  - Automatically identifies categorical vs numerical features
  - Trains a `RandomForestRegressor` or `RandomForestClassifier` per column
  - Saves trained models into `notebooks/impute_models/`
  - Applies predictions to fill missing values
- **Command:** `fill_null_ML(df)` auto-triggers column-wise ML imputation

### 3. ğŸ—ƒï¸ Load to PostgreSQL

- **Script:** `scripts/load_to_postgres.py`
- **Library:** `SQLAlchemy`
- **Database:** `PostgreSQL` (assumes local setup)
- **Table:** `trending_videos`
- **Note:** You must replace the DB connection URI with your credentials

### 4. â˜ï¸ Upload to AWS S3

- **Script:** `scripts/upload_to_s3.py`
- **Library:** `boto3`
- **Bucket Name:** `youtube-trending-data-pipeline`
- **Environment:** Assumes AWS credentials are available via environment or `~/.aws/credentials`

---

## â±ï¸ Workflow Orchestration with Airflow

- **DAG:** `airflow/dags/youtube_pipeline.py`
- **Services:** Airflow webserver, scheduler, and PostgreSQL via `airflow/docker-compose.yml`
- **Pipeline Tasks:**
  - `fetch_data` â†’ `clean_with_ML` â†’ `upload_to_postgres` â†’ `upload_to_s3`

**Run Airflow:**

```bash
cd airflow
docker-compose up
```

## ğŸš¢ Dockerized Environment

To make the project reproducible and easy to deploy, it is containerized using **Docker** and orchestrated using **Docker Compose**.

### ğŸ“„ `Dockerfile`

The `Dockerfile` sets up the Python environment and installs all necessary dependencies (as listed in `requirements.txt`), including:

- `pandas`, `numpy`, `scikit-learn` for data processing and ML
- `boto3` for AWS S3 interactions
- `sqlalchemy` for PostgreSQL loading
- `kaggle` API for dataset download
- `apache-airflow` for orchestration

### ğŸ§± `docker-compose.yml`

The `docker-compose.yml` file spins up the following services:

- **Airflow Scheduler**
- **Airflow Webserver**
- **Airflow Worker**
- **PostgreSQL Database**
- **Airflow Metadata Database**

All containers are networked together for seamless integration and automated pipeline execution.

### ğŸ”§ How to Run Docker Compose

```bash
docker-compose up --build
```

Then visit Airflow UI at: [http://localhost:8080](http://localhost:8080)

## ğŸ“¡ Airflow DAG: `youtube_pipeline.py`

The DAG defined in `airflow/dags/youtube_pipeline.py` automates the full ETL process:

1. **Fetch Data from Kaggle**  
   Downloads and extracts YouTube trending data using the Kaggle API.

2. **Clean & Impute Missing Values**  
   Uses `DataCleaning.py` to preprocess the dataset and fill missing values via ML models.

3. **Upload Clean Data to AWS S3**  
   Transfers the cleaned `.csv` to your S3 bucket using `upload_to_s3.py`.

4. **Load into PostgreSQL**  
   Inserts data into a PostgreSQL table using `load_to_postgres.py`.

### ğŸ” Schedule

You can customize the frequency of the DAG from within the DAG definition (`schedule_interval`). This enables automated, periodic data ingestion and cleaning.

## ğŸ’¡ Machine Learning-based Data Cleaning

To go beyond basic cleaning, the project uses **ML models to impute missing values** for both numerical and categorical columns.

### Main Script: `notebooks/DataCleaning.py`

- Detects column types (categorical or numerical).
- Trains either a `RandomForestClassifier` or `RandomForestRegressor` based on column type.
- Uses only columns with no missing values as features.
- Serializes trained models using `joblib`.
- Applies trained models to predict missing values.

### Benefits:
- More accurate and intelligent filling of missing data.
- Dynamic and scalable â€” works on any DataFrame with minimal setup.

## â˜ï¸ AWS S3 Integration

The pipeline supports cloud data storage using **Amazon S3**.

### ğŸ” Authentication

Make sure to set up your AWS credentials in `~/.aws/credentials` or via environment variables:

```bash
export AWS_ACCESS_KEY_ID=your_key
export AWS_SECRET_ACCESS_KEY=your_secret
```

### â˜ï¸ Bucket Upload

The `upload_to_s3.py` script uploads the cleaned dataset to:

```
s3://youtube-trending-data-pipeline/youtube_trending.csv
```

## ğŸ—ƒï¸ PostgreSQL Integration

We use **SQLAlchemy** to push the cleaned dataset into a PostgreSQL table.

- Connection string:
```
postgresql://user:password@localhost:5432/youtube
```

- Table name:
```
trending_videos
```

### ğŸ“„ Schema Definition

See `sql/create_tables.sql` for manual schema setup, if needed.

## ğŸ“Š Jupyter Notebooks

- Exploratory analysis and visualization are handled in `notebooks/analysis.ipynb`.
- You can use it to:
  - Analyze view counts, likes/dislikes
  - Study trends by country, category
  - Detect outliers or bot behavior

## ğŸ” Kaggle API Integration

To use Kaggleâ€™s API:

1. Go to [Kaggle Account Settings](https://www.kaggle.com/account)
2. Click on "Create New API Token"
3. Save the downloaded `kaggle.json` file to `.kaggle/` or `/root/.kaggle/` inside Docker
4. Authenticate using:

```python
from kaggle.api.kaggle_api_extended import KaggleApi
api = KaggleApi()
api.authenticate()
```

## ğŸ§ª Requirements

All dependencies are listed in `requirements.txt`. You can install them manually with:

```bash
pip install -r requirements.txt
```

Or rely on Docker to handle the environment setup.

## ğŸš€ Project Structure Overview

```
â””â”€â”€ ğŸ“YoutubeTrends_DataEngineering
        â””â”€â”€ config
        â””â”€â”€ description
        â””â”€â”€ FETCH_HEAD
        â””â”€â”€ HEAD
        â””â”€â”€ ğŸ“hooks
            â””â”€â”€ applypatch-msg.sample
            â””â”€â”€ commit-msg.sample
            â””â”€â”€ fsmonitor-watchman.sample
            â””â”€â”€ post-update.sample
            â””â”€â”€ pre-applypatch.sample
            â””â”€â”€ pre-commit.sample
            â””â”€â”€ pre-merge-commit.sample
            â””â”€â”€ pre-push.sample
            â””â”€â”€ pre-rebase.sample
            â””â”€â”€ pre-receive.sample
            â””â”€â”€ prepare-commit-msg.sample
            â””â”€â”€ push-to-checkout.sample
            â””â”€â”€ update.sample
        â””â”€â”€ ğŸ“info
            â””â”€â”€ exclude
        â””â”€â”€ ğŸ“objects
            â””â”€â”€ ğŸ“info
            â””â”€â”€ ğŸ“pack
        â””â”€â”€ ğŸ“refs
            â””â”€â”€ ğŸ“heads
            â””â”€â”€ ğŸ“tags
    â””â”€â”€ ğŸ“.kaggle
        â””â”€â”€ kaggle.json
    â””â”€â”€ ğŸ“airflow
        â””â”€â”€ ğŸ“dags
            â””â”€â”€ youtube_pipeline.py
        â””â”€â”€ docker-compose.yml
    â””â”€â”€ ğŸ“data
        â””â”€â”€ CA_category_id.json
        â””â”€â”€ CAvideos.csv
        â””â”€â”€ DE_category_id.json
        â””â”€â”€ DEvideos.csv
        â””â”€â”€ FR_category_id.json
        â””â”€â”€ FRvideos.csv
        â””â”€â”€ GB_category_id.json
        â””â”€â”€ GBvideos.csv
        â””â”€â”€ IN_category_id.json
        â””â”€â”€ INvideos.csv
        â””â”€â”€ JP_category_id.json
        â””â”€â”€ JPvideos.csv
        â””â”€â”€ KR_category_id.json
        â””â”€â”€ KRvideos.csv
        â””â”€â”€ MX_category_id.json
        â””â”€â”€ MXvideos.csv
        â””â”€â”€ RU_category_id.json
        â””â”€â”€ RUvideos.csv
        â””â”€â”€ US_category_id.json
        â””â”€â”€ USvideos.csv
        â””â”€â”€ youtube_trending.csv
        â””â”€â”€ youtube-new.zip
    â””â”€â”€ ğŸ“kaggle
        â””â”€â”€ .DS_Store
        â””â”€â”€ kaggle.json
    â””â”€â”€ ğŸ“notebooks
        â””â”€â”€ ğŸ“__pycache__
            â””â”€â”€ DataCleaning.cpython-311.pyc
        â””â”€â”€ analysis.ipynb
        â””â”€â”€ DataCleaning.py
        â””â”€â”€ ğŸ“impute_models
    â””â”€â”€ ğŸ“scripts
        â””â”€â”€ .DS_Store
        â””â”€â”€ .gitignore
        â””â”€â”€ fetch_data.py
        â””â”€â”€ load_to_postgres.py
        â””â”€â”€ upload_to_s3.py
    â””â”€â”€ ğŸ“sql
        â””â”€â”€ create_tables.sql
    â””â”€â”€ .DS_Store
    â””â”€â”€ .gitignore
    â””â”€â”€ docker-compose.yml
    â””â”€â”€ Dockerfile
    â””â”€â”€ README.md
    â””â”€â”€ requirements.txt
```

## ğŸ“Œ Next Steps & Ideas

- Add unit tests and logging
- Build a dashboard (e.g., using Streamlit or Superset)
- Apply anomaly detection on trending metrics
- Schedule weekly Airflow runs for continuous monitoring

## ğŸ¤ Contributing

Feel free to fork, open issues, and submit pull requests!

## ğŸ“ License

This project is licensed under the MIT License.
